{
 "metadata": {
  "name": "",
  "signature": "sha256:16c6533cea7719a270827dcc8e5366d1c29b4cd8b972dd0a737527f68686b0dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on http://neuralnetworksanddeeplearning.com/chap2.html\n",
      "and Andrew Ng ML Course from Coursera\n",
      "\n",
      "$(x)^{10}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"perceptron.jpg\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Install MathJax to use $Latex$ locally\n",
      "\n",
      "```python\n",
      "from IPython.external.mathjax import install_mathjax\n",
      "install_mathjax()```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fazer um neuron\n",
      "Feedforward\n",
      "Tenho as entradas e os pesos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inputs = [np.array([[0], [0]]),np.array([[0], [1]]),np.array([[1], [0]]),np.array([[1], [1]])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bias = np.array([[1],[1],[1],[1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Weights\n",
      "w = np.array([20, 20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    ''' Sigmoid activation function '''\n",
      "    return 1.0/(1.0+np.exp(-z))\n",
      "# Vectorize the function\n",
      "sigmoid_vec = np.vectorize(sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = np.dot(w, inputs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigmoid_vec(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([[ 0.5],\n",
        "       [ 1. ],\n",
        "       [ 1. ],\n",
        "       [ 1. ]])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feedforward(a, w):\n",
      "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
      "        a = sigmoid_vec(np.dot(w, a))\n",
      "        return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feedforward(inputs, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "array([[ 0.5],\n",
        "       [ 1. ],\n",
        "       [ 1. ],\n",
        "       [ 1. ]])"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# NN com 3 camadas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"Escrita.png\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## FeedForward"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tamanho dos Layers\n",
      "sizes = [2, 2, 1]\n",
      "num_layers = len(sizes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Weights created with Easy Neurons\n",
      "# [[-3.43327, -3.59466],[-3.47137, -3.55708]] / [[-6.94198, -7.28447]]\n",
      "weights = [np.array([[-3.43327, -3.59466],[-3.47137, -3.55708]]), np.array([[-6.94198, -7.28447]])]\n",
      "# Biases\n",
      "# [[4.55499, 4.75544]] / [[5.69759]]\n",
      "biases = [np.array([[4.55499, 4.75544]]), np.array([[5.69759]])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fazer o calculo da saida de cada camada\n",
      "output = [] # Keep the output for each training sample\n",
      "for inpu in inputs: # C\u00e1lculo para cada Training sample\n",
      "    activation = inpu\n",
      "    #output = [activation]\n",
      "    #zs = []\n",
      "    ### Feedforward calculation\n",
      "    # Calculate each training sample output\n",
      "    for b, w in zip(biases, weights):\n",
      "        a = sigmoid_vec(np.dot(w, activation) + b)\n",
      "    output.append(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[array([[ 0.99665717]]),\n",
        " array([[ 0.16982331]]),\n",
        " array([[ 0.22367277]]),\n",
        " array([[ 0.00019764]])]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Network Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Backpropagation happens from backward. It derives the Cost Function according to the weights and finds the weights changes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Backprop Steps__\n",
      "1. Training sample\n",
      "2. Set $\\Delta$ to zero\n",
      "3. Iterate through training set $1:m$\n",
      "4. Perform feedforward and calculate Activations ($a_{l}$) for each Layer\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "#import bigfloat as bf\n",
      "#bigfloat.exp(5000,bigfloat.precision(100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tools\n",
      "def sigmoid(z):\n",
      "    ''' Sigmoid activation function '''\n",
      "    return 1.0/(1.0 + np.exp(-z))\n",
      "# Vectorize the function\n",
      "sigmoid_vec = np.vectorize(sigmoid)\n",
      "\n",
      "def cost_derivative(output_activations, y):\n",
      "    return (output_activations - y)\n",
      "\n",
      "def sigmoid_prime(z):\n",
      "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
      "    return sigmoid(z)*(1-sigmoid(z))\n",
      "\n",
      "sigmoid_prime_vec = np.vectorize(sigmoid_prime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Backprop function\n",
      "def backprop(x, y, weights, biases): # Entradas: Training sample\n",
      "    # No c\u00f3digo do site n\u00e3o entra os weights e biases pois \u00e9 uma classe que recebe estes com o SELF\n",
      "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
      "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
      "    # feedforward\n",
      "    # feedforward\n",
      "    activation = x\n",
      "    activations = [x] # list to store all the activations, layer by layer\n",
      "    zs = [] # list to store all the z vectors, layer by layer\n",
      "    for b, w in zip(biases, weights):\n",
      "        z = np.dot(w, activation)+b\n",
      "        zs.append(z)\n",
      "        activation = sigmoid_vec(z)\n",
      "        activations.append(activation)\n",
      "        # backward pass\n",
      "    delta = cost_derivative(activations[-1], y) * sigmoid_prime_vec(zs[-1])\n",
      "    nabla_b[-1] = delta\n",
      "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
      "    # Note that the variable l in the loop below is used a little\n",
      "    # differently to the notation in Chapter 2 of the book.  Here,\n",
      "    # l = 1 means the last layer of neurons, l = 2 is the\n",
      "    # second-last layer, and so on.  It's a renumbering of the\n",
      "    # scheme in the book, used here to take advantage of the fact\n",
      "    # that Python can use negative indices in lists.\n",
      "    for l in xrange(2, num_layers):\n",
      "        z = zs[-l]\n",
      "        spv = sigmoid_prime_vec(z)\n",
      "        delta = np.dot(weights[-l+1].transpose(), delta) * spv\n",
      "        nabla_b[-l] = delta\n",
      "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
      "    return (nabla_b, nabla_w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_weights(mini_batch, eta, weights, biases):\n",
      "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
      "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
      "    for x, y in mini_batch:\n",
      "        delta_nabla_b, delta_nabla_w = backprop(x, y, weights, biases)\n",
      "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
      "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
      "        weights = [w-(eta/len(mini_batch))*nw \n",
      "                            for w, nw in zip(weights, nabla_w)]\n",
      "        biases = [b-(eta/len(mini_batch))*nb \n",
      "                           for b, nb in zip(biases, nabla_b)]\n",
      "    return (weights, biases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluation(inputs, weights, biases):\n",
      "    output = [] # Keep the output for each training sample\n",
      "    for inpu in inputs: # C\u00e1lculo para cada Training sample\n",
      "        activation = inpu\n",
      "        for b, w in zip(biases, weights):\n",
      "            activation = sigmoid_vec(np.dot(w, activation) + b)\n",
      "        output.append(activation)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inputs and Targets\n",
      "inputs = [np.array([[0], [0]]),np.array([[0], [1]]),np.array([[1], [0]]),np.array([[1], [1]])]\n",
      "target = [np.array([0]), np.array([0]), np.array([0]), np.array([1])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Network Architecture\n",
      "sizes = [2, 2, 1]\n",
      "num_layers = len(sizes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random initialization\n",
      "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
      "biases = [np.random.randn(y, 1) for y in sizes[1:]] # Biases weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Driksel says that $\\eta$ (learning rate) must be between $0.01 \\leq \\eta \\leq 0.9$, but Nielsen uses\n",
      "$\\eta = 3.0$ in his book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Definitions and accessory variables\n",
      "w = weights\n",
      "b = biases\n",
      "saida = []\n",
      "mini_batch = zip(inputs, target)\n",
      "eta = 5.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run epochs for training\n",
      "for j in xrange(2000): # epochs\n",
      "    [w, b] = update_weights(mini_batch, eta, w, b)\n",
      "print evaluation(inputs, w, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[ 0.00060206]]), array([[ 0.00841434]]), array([[ 0.01016986]]), array([[ 0.97985896]])]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Improving learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use Cross-entropy cost function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Onde muda em rela\u00e7\u00e3o ao m\u00e9todo anterior?\n",
      "1. Backprop\n",
      "2. F\u00f3rmulas de update dos weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}