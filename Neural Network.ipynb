{
 "metadata": {
  "name": "",
  "signature": "sha256:503e14e8e069205ae63cc337e8956537e2b2e1e43f66fa85ce6b2585f8ff3e75"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on http://neuralnetworksanddeeplearning.com/chap2.html\n",
      "and Andrew Ng ML Course from Coursera\n",
      "\n",
      "$(x)^{10}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"perceptron.jpg\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Install MathJax to use $Latex$ locally\n",
      "\n",
      "```python\n",
      "from IPython.external.mathjax import install_mathjax\n",
      "install_mathjax()```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fazer um neuron\n",
      "Feedforward\n",
      "Tenho as entradas e os pesos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inputs = [np.array([0, 0]),np.array([0, 1]),np.array([1, 0]),np.array([1, 1])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bias = np.array([[1],[1],[1],[1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Weights\n",
      "w = np.array([-30, 20, 20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    ''' Sigmoid activation function '''\n",
      "    return 1.0/(1.0+np.exp(-z))\n",
      "# Vectorize the function\n",
      "sigmoid_vec = np.vectorize(sigmoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = np.dot(w, inputs.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'np' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-0f75f50684cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigmoid_vec(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "array([  9.35762297e-14,   4.53978687e-05,   4.53978687e-05,\n",
        "         9.99954602e-01])"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feedforward(a, w):\n",
      "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
      "        a = sigmoid_vec(np.dot(w, a.T))\n",
      "        return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feedforward(a, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([  9.35762297e-14,   4.53978687e-05,   4.53978687e-05,\n",
        "         9.99954602e-01])"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# NN com 3 camadas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"Escrita.png\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## FeedForward"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tamanho dos Layers\n",
      "sizes = [2, 2, 1]\n",
      "num_layers = len(sizes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Weights created with Easy Neurons\n",
      "# [[-3.43327, -3.59466],[-3.47137, -3.55708]] / [[-6.94198, -7.28447]]\n",
      "weights = [np.array([[-3.43327, -3.59466],[-3.47137, -3.55708]]), np.array([[-6.94198, -7.28447]])]\n",
      "# Biases\n",
      "# [[4.55499, 4.75544]] / [[5.69759]]\n",
      "biases = [np.array([[4.55499, 4.75544]]), np.array([[5.69759]])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'np' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-d9d27c3dda79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Weights created with Easy Neurons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# [[-3.43327, -3.59466],[-3.47137, -3.55708]] / [[-6.94198, -7.28447]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3.43327\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m3.59466\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3.47137\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m3.55708\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6.94198\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m7.28447\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Biases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# [[4.55499, 4.75544]] / [[5.69759]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fazer o calculo da saida de cada camada\n",
      "output = [] # Keep the output for each training sample\n",
      "for inpu in inputs: # C\u00e1lculo para cada Training sample\n",
      "    activation = inpu\n",
      "    #output = [activation]\n",
      "    #zs = []\n",
      "    ### Feedforward calculation\n",
      "    # Calculate each training sample output\n",
      "    for b, w in zip(biases, weights):\n",
      "        a = sigmoid_vec(np.dot(w, activation) + b)\n",
      "    output.append(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[array([[ 0.99665717]]),\n",
        " array([[ 0.16982331]]),\n",
        " array([[ 0.22367277]]),\n",
        " array([[ 0.00019764]])]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Network Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Backpropagation happens from backward. It derives the Cost Function according to the weights and finds the weights changes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Backprop Steps__\n",
      "1. Training sample\n",
      "2. Set $\\Delta$ to zero\n",
      "3. Iterate through training set $1:m$\n",
      "4. Perform feedforward and calculate Activations ($a_{l}$) for each Layer\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "#import bigfloat as bf\n",
      "#bigfloat.exp(5000,bigfloat.precision(100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tools\n",
      "def sigmoid(z):\n",
      "    ''' Sigmoid activation function '''\n",
      "    return 1.0/(1.0 + np.exp(-z))\n",
      "# Vectorize the function\n",
      "sigmoid_vec = np.vectorize(sigmoid)\n",
      "\n",
      "def cost_derivative(output_activations, y):\n",
      "    return (output_activations - y)\n",
      "\n",
      "def sigmoid_prime(z):\n",
      "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
      "    return sigmoid(z)*(1-sigmoid(z))\n",
      "\n",
      "sigmoid_prime_vec = np.vectorize(sigmoid_prime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Backprop function\n",
      "def backprop(x, y, weights, biases): # Entradas: Training sample\n",
      "    # No c\u00f3digo do site n\u00e3o entra os weights e biases pois \u00e9 uma classe que recebe estes com o SELF\n",
      "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
      "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
      "    # feedforward\n",
      "    # feedforward\n",
      "    activation = x\n",
      "    activations = [x] # list to store all the activations, layer by layer\n",
      "    zs = [] # list to store all the z vectors, layer by layer\n",
      "    for b, w in zip(biases, weights):\n",
      "        z = np.dot(w, activation)+b\n",
      "        zs.append(z)\n",
      "        activation = sigmoid_vec(z)\n",
      "        activations.append(activation)\n",
      "        # backward pass\n",
      "    delta = cost_derivative(activations[-1], y) * sigmoid_prime_vec(zs[-1])\n",
      "    nabla_b[-1] = delta\n",
      "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
      "    # Note that the variable l in the loop below is used a little\n",
      "    # differently to the notation in Chapter 2 of the book.  Here,\n",
      "    # l = 1 means the last layer of neurons, l = 2 is the\n",
      "    # second-last layer, and so on.  It's a renumbering of the\n",
      "    # scheme in the book, used here to take advantage of the fact\n",
      "    # that Python can use negative indices in lists.\n",
      "    for l in xrange(2, num_layers):\n",
      "        z = zs[-l]\n",
      "        spv = sigmoid_prime_vec(z)\n",
      "        delta = np.dot(weights[-l+1].transpose(), delta) * spv\n",
      "        nabla_b[-l] = delta\n",
      "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
      "    return (nabla_b, nabla_w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_weights(mini_batch, eta, weights, biases):\n",
      "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
      "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
      "    for x, y in mini_batch:\n",
      "        delta_nabla_b, delta_nabla_w = backprop(x, y, weights, biases)\n",
      "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
      "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
      "        weights = [w-(eta/len(mini_batch))*nw \n",
      "                            for w, nw in zip(weights, nabla_w)]\n",
      "        biases = [b-(eta/len(mini_batch))*nb \n",
      "                           for b, nb in zip(biases, nabla_b)]\n",
      "    return (weights, biases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluation(inputs, weights, biases):\n",
      "    output = [] # Keep the output for each training sample\n",
      "    for inpu in inputs: # C\u00e1lculo para cada Training sample\n",
      "        activation = inpu\n",
      "        for b, w in zip(biases, weights):\n",
      "            activation = sigmoid_vec(np.dot(w, activation) + b)\n",
      "        output.append(activation)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inputs and Targets\n",
      "inputs = [np.array([[0], [0]]),np.array([[0], [1]]),np.array([[1], [0]]),np.array([[1], [1]])]\n",
      "target = [np.array([0]), np.array([0]), np.array([0]), np.array([1])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Network Architecture\n",
      "sizes = [2, 2, 1]\n",
      "num_layers = len(sizes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random initialization\n",
      "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
      "biases = [np.random.randn(y, 1) for y in sizes[1:]] # Biases weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Driksel says that $\\eta$ (learning rate) must be between $0.01 \\leq \\eta \\leq 0.9$, but Nielsen uses\n",
      "$\\eta = 3.0$ in his book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Definitions and accessory variables\n",
      "w = weights\n",
      "b = biases\n",
      "saida = []\n",
      "mini_batch = zip(inputs, target)\n",
      "eta = 5.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run epochs for training\n",
      "for j in xrange(2000): # epochs\n",
      "    [w, b] = update_weights(mini_batch, eta, w, b)\n",
      "print evaluation(inputs, w, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[ 0.00140111]]), array([[ 0.00766394]]), array([[ 0.00923886]]), array([[ 0.97930168]])]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Improve learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use Cross-entropy cost function"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}